{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import CustomPreprocessor, ModelSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping duplicates 12139\n",
      "After dropping duplicates 8573\n"
     ]
    }
   ],
   "source": [
    "df_sentences = pd.read_csv('../../raw_data/labeled_sentences_merged_imbalanced_12k_extra_classes.csv',usecols=['sentence','topic_label'])\n",
    "print(f'Before dropping duplicates {len(df_sentences)}')\n",
    "df_sentences.drop_duplicates(inplace=True)\n",
    "print(f'After dropping duplicates {len(df_sentences)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'average': False,\n",
       " 'class_weight': None,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 0.1,\n",
       " 'eta0': 0.0,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'learning_rate': 'optimal',\n",
       " 'loss': 'hinge',\n",
       " 'max_iter': 1000,\n",
       " 'n_iter_no_change': 5,\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'tol': 0.001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', CustomPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.9)),\n",
    "    ('model',SGDClassifier(loss='hinge'))\n",
    "])\n",
    "SGDClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessor', CustomPreprocessor()),\n",
       "  ('vectorizer', TfidfVectorizer(max_df=0.9)),\n",
       "  ('model', SGDClassifier())],\n",
       " 'verbose': False,\n",
       " 'preprocessor': CustomPreprocessor(),\n",
       " 'vectorizer': TfidfVectorizer(max_df=0.9),\n",
       " 'model': SGDClassifier(),\n",
       " 'preprocessor__accents': 'keep',\n",
       " 'preprocessor__html': 'keep',\n",
       " 'preprocessor__lemma': False,\n",
       " 'preprocessor__negation': 'keep',\n",
       " 'preprocessor__numbers': 'remove',\n",
       " 'preprocessor__punctuation': 'remove',\n",
       " 'preprocessor__remove_stopwords': True,\n",
       " 'preprocessor__stem': True,\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.float64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 0.9,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 1,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__norm': 'l2',\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__smooth_idf': True,\n",
       " 'vectorizer__stop_words': None,\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__sublinear_tf': False,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__use_idf': True,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'model__alpha': 0.0001,\n",
       " 'model__average': False,\n",
       " 'model__class_weight': None,\n",
       " 'model__early_stopping': False,\n",
       " 'model__epsilon': 0.1,\n",
       " 'model__eta0': 0.0,\n",
       " 'model__fit_intercept': True,\n",
       " 'model__l1_ratio': 0.15,\n",
       " 'model__learning_rate': 'optimal',\n",
       " 'model__loss': 'hinge',\n",
       " 'model__max_iter': 1000,\n",
       " 'model__n_iter_no_change': 5,\n",
       " 'model__n_jobs': None,\n",
       " 'model__penalty': 'l2',\n",
       " 'model__power_t': 0.5,\n",
       " 'model__random_state': None,\n",
       " 'model__shuffle': True,\n",
       " 'model__tol': 0.001,\n",
       " 'model__validation_fraction': 0.1,\n",
       " 'model__verbose': 0,\n",
       " 'model__warm_start': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sentences[['sentence']].astype(str)\n",
    "y_topic = df_sentences.loc[:,'topic_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_topic_train, X_topic_test, y_topic_train, y_topic_test = train_test_split(X,y_topic,test_size=0.25,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 5/5] END ..................................., score=0.727 total time=  11.4s\n",
      "[CV 4/5] END ..................................., score=0.737 total time=  11.4s\n",
      "[CV 1/5] END ..................................., score=0.724 total time=  11.5s\n",
      "[CV 3/5] END ..................................., score=0.757 total time=  11.6s\n",
      "[CV 2/5] END ..................................., score=0.739 total time=  11.7s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    # 'model__loss' : ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron']\n",
    "    \n",
    "}\n",
    "\n",
    "topic_grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1,scoring='f1_weighted',verbose=3,error_score='raise').fit(X_topic_train, y_topic_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7370244591295634"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg-scoring-lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0da4683c4fad0afeec93b8b497065776641c10538c023e7f59ac79d1ec6963bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
